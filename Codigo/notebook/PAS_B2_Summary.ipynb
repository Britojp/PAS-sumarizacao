{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exSfiyVCrkED"
      },
      "source": [
        "1º Parte: Receber o arquivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "qHljSgypJM6k",
        "outputId": "aab61f62-b8fc-452b-c3a6-39b9a8d96947"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c08356f0-3fd1-49e5-aaf1-29bba5fdae52\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c08356f0-3fd1-49e5-aaf1-29bba5fdae52\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving BATALHA DOS MEMES.mp4 to BATALHA DOS MEMES.mp4\n"
          ]
        }
      ],
      "source": [
        "### Receber o video no colab (Upload)\n",
        "from google.colab import files\n",
        "uploaded = files.upload() # Até ~100MB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oY7DKzk5eIaa"
      },
      "outputs": [],
      "source": [
        "### Receber o video da database\n",
        "# https://www.kaggle.com/datasets/kunwardeepak/youtube-trending-august-2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N83AJ36LrqIV"
      },
      "source": [
        "2º Parte: Processar o arquivo (audio e frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9gdEK0n3jPl",
        "outputId": "5bec39ef-2b95-4610-fef4-9201dcc5870a"
      },
      "outputs": [],
      "source": [
        "### Verificar a duração do video para restrições de frames e audio\n",
        "!pip install ffmpeg\n",
        "import ffmpeg\n",
        "import subprocess\n",
        "\n",
        "# Retorna a duração em segundos\n",
        "def verificarDuracaoVideo(videoEntrada):\n",
        "  try:\n",
        "    commando = [\n",
        "      'ffprobe',\n",
        "      '-i', videoEntrada,\n",
        "      '-show_entries', 'format=duration',\n",
        "      '-v', 'quiet',\n",
        "      '-of', 'csv=p=0'\n",
        "    ]\n",
        "    # Executar o comando\n",
        "    resultado = subprocess.run(commando, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "    stderr = resultado.stderr.decode('utf-8')\n",
        "    if stderr:\n",
        "      print(f\"Erro do ffprobe: {stderr}\")\n",
        "    # Ler a duração\n",
        "    duracao = float(resultado.stdout.decode('utf-8').strip())\n",
        "    return duracao\n",
        "  except Exception as e:\n",
        "    print(f\"Erro ao converter a duração: {e}\")\n",
        "    return None\n",
        "\n",
        "\n",
        "videoEntrada = None\n",
        "# Encontrar o video\n",
        "import os\n",
        "for arquivo in os.listdir():\n",
        "  if arquivo.endswith(\".mp4\"):\n",
        "    videoEntrada = arquivo\n",
        "    break\n",
        "\n",
        "if videoEntrada:\n",
        "  # Verificar a duração do video\n",
        "  duracaoVideo = verificarDuracaoVideo(videoEntrada)\n",
        "  print(videoEntrada,duracaoVideo)\n",
        "else:\n",
        "  print('Nenhum video encontrado!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnDXNgZ8LANg",
        "outputId": "7ae0e2e3-dadf-415c-c85f-b6f05545887a"
      },
      "outputs": [],
      "source": [
        "### Extrair o audio do video\n",
        "!pip install ffmpeg-python\n",
        "\n",
        "import ffmpeg\n",
        "import subprocess\n",
        "\n",
        "def extrair_audio(videoEntrada, localAudioSaida):\n",
        "  try:\n",
        "    subprocess.run([\n",
        "      'ffmpeg', '-i', videoEntrada,\n",
        "      '-vn', '-acodec', 'libmp3lame', '-ab', '192k', localAudioSaida\n",
        "    ], check=False)\n",
        "    print(\"Audio extraido!\")\n",
        "  except Exception as e:\n",
        "    print(f\"Erro: {e}\")\n",
        "\n",
        "videoEntrada = None\n",
        "# Encontrar o video\n",
        "import os\n",
        "for arquivo in os.listdir():\n",
        "  if arquivo.endswith(\".mp4\"):\n",
        "    videoEntrada = arquivo\n",
        "    break\n",
        "\n",
        "if videoEntrada:\n",
        "  localAudioSaida = videoEntrada.replace('.mp4', '.mp3')\n",
        "  extrair_audio(videoEntrada, localAudioSaida)\n",
        "else:\n",
        "  print('Nenhum video encontrado!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wATlD6KBqwx4"
      },
      "outputs": [],
      "source": [
        "### Se necessário, repartir o audio em partes menores\n",
        "!pip install ffmpeg-python\n",
        "import ffmpeg\n",
        "import os\n",
        "\n",
        "def dividirAudio(audioEntrada, localAudioSaida, tamanhoMaximo = None):\n",
        "  # Se não existir -> criar a pasta\n",
        "  if not os.path.exists(localAudioSaida):\n",
        "    os.makedirs(localAudioSaida)\n",
        "  # Tentar dividir o audio\n",
        "  try:\n",
        "    ffmpeg.input(audioEntrada).output(f\"{localAudioSaida}/chunk%03d.mp3\", f='segment', segment_time=tamanhoMaximo).run()\n",
        "    print(\"Audio dividido!\")\n",
        "  except ffmpeg.Error as e:\n",
        "    print(f\"Erro: {e.stderr.decode()}\")\n",
        "\n",
        "# Encontrar o audio\n",
        "audioEntrada = None\n",
        "for arquivo in os.listdir():\n",
        "  if arquivo.endswith(\".mp3\"):\n",
        "    audioEntrada = arquivo\n",
        "    break\n",
        "\n",
        "if audioEntrada:\n",
        "  dividirAudio(audioEntrada,'/content/Audio/', 2*60)\n",
        "else:\n",
        "  print('Nenhum audio encontrado!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqWAdTNcAiK7",
        "outputId": "8e61e8bf-5d9c-4fd2-99b1-7d4e0d3a10cc"
      },
      "outputs": [],
      "source": [
        "### Extrair os frames do video\n",
        "!pip install opencv-python\n",
        "import cv2\n",
        "import time\n",
        "import os\n",
        "\n",
        "def extrairFrames(videoEntrada, localFramesSaida):\n",
        "  try:\n",
        "    # Se não existir -> criar a pasta\n",
        "    if not os.path.exists(localFramesSaida):\n",
        "      os.makedirs(localFramesSaida)\n",
        "\n",
        "    start_time = time.time() # Para o cronometro\n",
        "\n",
        "    cap = cv2.VideoCapture(videoEntrada)\n",
        "    frame_count = 0\n",
        "    frame_counter = 0\n",
        "    # Determinar intervalo da captura\n",
        "    fps = 1\n",
        "    duracaoMaximaVideo = 5*60\n",
        "    intervaloExtracao = 1.0/fps\n",
        "    if duracaoVideo > duracaoMaximaVideo:\n",
        "      intervaloExtracao = duracaoVideo/(fps*duracaoMaximaVideo)\n",
        "    # Video em si\n",
        "    while cap.isOpened():\n",
        "      ret, frame = cap.read()\n",
        "      if not ret:\n",
        "          break\n",
        "\n",
        "      current_time = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0  # Get timestamp in seconds\n",
        "\n",
        "      # Salvar o frame a cada {intervaloExtracao}\n",
        "      if current_time >= frame_count * intervaloExtracao:\n",
        "          frame_filename = os.path.join(localFramesSaida, f\"frame_{frame_counter:04}_time_{current_time:.2f}.png\")\n",
        "          cv2.imwrite(frame_filename, frame)\n",
        "          frame_counter += 1\n",
        "          frame_count += 1\n",
        "\n",
        "      # Break para quando o video ter acabado\n",
        "      if current_time > cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0:\n",
        "          break\n",
        "\n",
        "    cap.release()\n",
        "    end_time = time.time() # Parar o cronometro\n",
        "    print('Frames extraidos')\n",
        "    print(f\"Tempo de execução: {end_time - start_time: .1f} segundos\")\n",
        "    print(f\"{(frame_counter+1)/(end_time-start_time): .2f} frames por segundo\")\n",
        "  except Exception as e:\n",
        "    print(f\"Erro: {e}\")\n",
        "\n",
        "# Encontrar o video\n",
        "videoEntrada = None\n",
        "for arquivo in os.listdir():\n",
        "  if arquivo.endswith(\".mp4\"):\n",
        "    videoEntrada = arquivo\n",
        "    break\n",
        "\n",
        "if videoEntrada:\n",
        "  extrairFrames(videoEntrada, '/content/Frames/Original/')\n",
        "else:\n",
        "  print('Nenhum video encontrado!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juGFtwclQjR9",
        "outputId": "85ff13a3-93b2-4b28-9e59-00427f5e62ac"
      },
      "outputs": [],
      "source": [
        "### Processar os frames\n",
        "!pip install Pillow\n",
        "from PIL import Image\n",
        "\n",
        "def processarFrames(localFrames):\n",
        "  try:\n",
        "    # Se não existir -> criar a pasta\n",
        "    if not os.path.exists('/content/Frames/Processados/'):\n",
        "        os.makedirs('/content/Frames/Processados/')\n",
        "    # Tentar processar os frames\n",
        "    for arquivo in os.listdir(localFrames):\n",
        "      if arquivo.endswith('.png'):\n",
        "        # Imagem recebida\n",
        "        with Image.open(os.path.join(localFrames,arquivo)) as img:\n",
        "          # Verificando tamanho da imagem\n",
        "          width, height = img.size\n",
        "          if width > 1280 or height > 720: # Reduzir tamanho\n",
        "            # Seguir proporção para evitar distorção\n",
        "            proporcao = width/height\n",
        "            if proporcao > 1: # Paisagem\n",
        "              img = img.resize((1280, int(1280/proporcao)), Image.Resampling.LANCZOS)\n",
        "            else: # Retrato\n",
        "              img = img.resize((int(720*proporcao), 720), Image.Resampling.LANCZOS)\n",
        "\n",
        "            # Salvar imagem na nova pasta\n",
        "            img.save(os.path.join('/content/Frames/Processados/', arquivo))\n",
        "    print('Frames processados')\n",
        "  except Exception as e:\n",
        "    print(f\"Erro: {e}\")\n",
        "\n",
        "# Tentar processar os frames\n",
        "import os\n",
        "if os.path.exists('/content/Frames/Original/'):\n",
        "  processarFrames('/content/Frames/Original/')\n",
        "else:\n",
        "  print('Nenhum frame encontrado!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vj3vDVL7_goR",
        "outputId": "e95353a2-dac3-4838-ba07-7a7089bcf5fe"
      },
      "outputs": [],
      "source": [
        "#temp\n",
        "!clear\n",
        "import os\n",
        "print(os.listdir())\n",
        "if os.path.exists('/content/Frames/Original/'):\n",
        "  print(sorted(os.listdir('/content/Frames/Original/')))\n",
        "  print(len(os.listdir('/content/Frames/Original/')),end='\\n------\\n')\n",
        "if os.path.exists('/content/Frames/Processados/'):\n",
        "  print(sorted(os.listdir('/content/Frames/Processados/')))\n",
        "  print(len(os.listdir('/content/Frames/Processados/')),end='\\n------\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izqWWCP-ryEY"
      },
      "source": [
        "3º Parte: Criar descrição e transcrição dos frames e áudio, respectivamente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4nTk-3ecujYG",
        "outputId": "275dd7d9-dbdf-46ae-e3fd-89578adc2e28"
      },
      "outputs": [],
      "source": [
        "### Instalar as bibliotecas maiores\n",
        "!pip install openai-whisper\n",
        "!pip install ollama\n",
        "!pip install torch transformers pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7HdN2KQKWk1S",
        "outputId": "c8380e39-01ed-46a1-f8a3-655c61d48b46"
      },
      "outputs": [],
      "source": [
        "### Transcrição do audio (Whisper x Vosk)\n",
        "## Whisper é mais preciso, mas é mais caro\n",
        "## Implementação inicial com whisper, se ficar inviável implementar Vosk\n",
        "\n",
        "## Whisper: Tempo de instalação = 2m 20s\n",
        "\n",
        "!pip install openai-whisper\n",
        "import whisper\n",
        "\n",
        "def transcreverAudio(audioEntrada):\n",
        "  try:\n",
        "    # Tentar transcrever o audio\n",
        "    model = whisper.load_model(\"base\")\n",
        "    result = model.transcribe(audioEntrada, language='pt')\n",
        "    print(\"Audio Extraido!\")\n",
        "    return result[\"text\"]\n",
        "  except Exception as e:\n",
        "    print(f\"Erro: {e}\")\n",
        "    return None\n",
        "\n",
        "audioEntrada = None\n",
        "for arquivo in os.listdir():\n",
        "  if arquivo.endswith(\".mp3\"):\n",
        "    audioEntrada = arquivo\n",
        "    break\n",
        "\n",
        "if audioEntrada:\n",
        "  transcricaoAudio = None # Salvar para caso de erro\n",
        "  transcricaoAudio = transcreverAudio(audioEntrada)\n",
        "else:\n",
        "  print('Nenhum audio encontrado!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSR9dBNrWpMj",
        "outputId": "488178b0-b052-4685-bb28-7f098442b153"
      },
      "outputs": [],
      "source": [
        "### Descrição dos frames\n",
        "## 135 frames em 14 minutos\n",
        "## Média de 5~7 segundos por frame\n",
        "!pip install torch transformers pillow\n",
        "\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "import os\n",
        "\n",
        "pastaFrames = \"/content/Frames/Processados/\" # Tentar usar os frames já processados\n",
        "\n",
        "try:\n",
        "    # Tenta configurar o modelo\n",
        "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "    # Descrição de 1 frame\n",
        "    def descrever_frame(caminho_imagem):\n",
        "        image = Image.open(caminho_imagem).convert(\"RGB\")\n",
        "\n",
        "        # Processa a imagem com o processador BLIP, retornando os tensores necessários para o modelo\n",
        "        inputs = processor(image, return_tensors=\"pt\")\n",
        "\n",
        "        # Gera a descrição da imagem a partir do modelo BLIP (max_length=100 significa que a descrição terá no máximo 100 tokens)\n",
        "        output = model.generate(**inputs, max_length=100)\n",
        "\n",
        "        # Decodifica a saída do modelo e converte para uma string legível\n",
        "        descricao = processor.decode(output[0], skip_special_tokens=True)\n",
        "        return descricao\n",
        "\n",
        "    # Lista com a descrição dos frames\n",
        "    def descreverFramesVideo(localFrames = '/content/Frames/Processados/'):\n",
        "      import time\n",
        "      listaDescricaoFrames = [] # List para armazenar as descrições\n",
        "      # Carregar vários frames\n",
        "      start_time = time.time()\n",
        "      for frame in sorted(os.listdir(localFrames)):\n",
        "          if frame.endswith((\".jpg\", \".png\")):\n",
        "              caminho = os.path.join(localFrames, frame)\n",
        "              descricao = descrever_frame(caminho)\n",
        "              # Dict para também armazenar o frame (por consequencia o timestamp)\n",
        "              infoFrame = {\n",
        "                  'frame': frame,\n",
        "                  'descricao': descricao,\n",
        "              }\n",
        "              listaDescricaoFrames.append(infoFrame)\n",
        "      end_time = time.time()\n",
        "      print('Descrição de frames criada!')\n",
        "      print(f\"{(end_time-start_time)/len(listaDescricaoFrames): .2f} segundos por frame\")\n",
        "      return listaDescricaoFrames\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Erro: {e}\")\n",
        "\n",
        "# Usar original se não foi necessário processar\n",
        "if not os.path.exists(pastaFrames): # Frames processados não existem -> usar os originais\n",
        "  pastaFrames = '/content/Frames/Original/'\n",
        "\n",
        "if videoEntrada:\n",
        "  listDescricoesFrames = None # Salvar para caso de erro\n",
        "  listDescricoesFrames = descreverFramesVideo(pastaFrames)\n",
        "else:\n",
        "  print('Nenhum video encontrado!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfW5ESbzd9iZ"
      },
      "source": [
        "4º Parte: Criação do sumário"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wvtD76xDBl9"
      },
      "source": [
        "Tentativa usando Deepseek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRm0pdxcWxD5"
      },
      "outputs": [],
      "source": [
        "### Tentativa de criação de contexto, criar uma descrição entre dois frames vizinhos (deepseek)\n",
        "######## INCOMPLETO, checar chaveAPI e analisaVizinhos\n",
        "######## Arrumar a localização para usar a variavel descricoesFrames\n",
        "!pip install requests\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# Configurar para o uso\n",
        "chaveAPI = \"\" # Adicionar uma chave\n",
        "url = \"https://api.deepseek.ai/v1/analyze\" # Alterar dependendo\n",
        "\n",
        "# Request headers com autorização\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {chaveAPI}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# Fazer a analise entre a descrição de dois frames\n",
        "def analisaVizinhosDeepseek(descricao1, descricao2):\n",
        "  conteudo = {\n",
        "      'descricao1': descricao1,\n",
        "      'descricao2': descricao2,\n",
        "  }\n",
        "  response = requests.post(url, headers=headers, json=conteudo)\n",
        "  # Verificando sucesso\n",
        "  if response.status_code == 200:\n",
        "      return response.json() # Tentar alterar para uma string\n",
        "      # Encontrar o campo que armazena o texto criado\n",
        "      #try:\n",
        "      #  # Extrai a string do campo apropriado do JSON\n",
        "      #  return response.json().get('texto', None)\n",
        "      #except ValueError:\n",
        "      #  return 'Erro na análise da resposta'\n",
        "  else:\n",
        "    print(f\"Error: {response.status_code}, {response.text}\")\n",
        "    return None\n",
        "\n",
        "# Criar a lista com as novas descrições\n",
        "def descricaoFramesVizinhosDeepseek():\n",
        "  listDescricoesVizinhos = []\n",
        "  listDescricoes = descricaoFrames() # Local já está com valor padrão correto\n",
        "  for i in range(0, len(listDescricoes) - 1, 2):\n",
        "    resposta = analisaVizinhosDeepseek(listDescricoes[i]['descricao'], listDescricoes[i+1]['descricao'])\n",
        "    if resposta:\n",
        "      # Armazenar as informações dos frames e a nova descrição\n",
        "      dictResposta = {\n",
        "          'frame1': listDescricoes[i]['frame'],\n",
        "          'frame2': listDescricoes[i+1]['frame'],\n",
        "          'descricao': resposta,\n",
        "      }\n",
        "      listDescricoesVizinhos.append(resposta)\n",
        "  # Com a lista criada:\n",
        "  if len(listDescricoesVizinhos) == len(listDescricoes) - 1: # Todos os elementos tiveram a descrição criada\n",
        "    print('Descrição contextualizada criada!')\n",
        "    return listDescricoesVizinhos\n",
        "  else: # Algum não foi criado\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CTLfRtxW-3T"
      },
      "outputs": [],
      "source": [
        "### Sumarizar a parte visual do video (deepseek)\n",
        "######## INCOMPLETO, checar chaveAPI\n",
        "!pip install requests\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# Configurar para o uso\n",
        "chaveAPI = \"\" # Adicionar uma chave\n",
        "url = \"https://api.deepseek.ai/v1/analyze\" # Alterar dependendo\n",
        "\n",
        "# Request headers com autorização\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {chaveAPI}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# Fazer a analise entre múltiplas descrições\n",
        "def analisaDescricoesFramesDeepseek(descricoes):\n",
        "    # Combine todas as descrições em um formato adequado para a API\n",
        "    conteudo = {'descricoes': descricoes}\n",
        "\n",
        "    response = requests.post(url, headers=headers, json=conteudo)\n",
        "\n",
        "    # Verificando sucesso\n",
        "    if response.status_code == 200:\n",
        "        return response.json()  # Retorna a resposta JSON completa\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}, {response.text}\")\n",
        "        return None\n",
        "\n",
        "# Criar a lista com as novas descrições\n",
        "def analiseFramesDeepseek():\n",
        "  listDescricoesVizinhos = []\n",
        "  #listDescricoes = descricaoFrames()  # Local já está com valor padrão correto\n",
        "  listDescricoes = descricaoFramesVizinhosDeepseek() # Tentar usar o texto contextualizado\n",
        "  # Agrupar todas as descrições em uma lista única\n",
        "  descricoes = [d['descricao'] for d in listDescricoes]\n",
        "  # Chamar a função de análise para todas as descrições\n",
        "  resposta = analisaDescricoesFramesDeepseek(descricoes)\n",
        "  if resposta:\n",
        "    print('Descrição contextualizada criada!')\n",
        "  return resposta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T16pagLaC5ES"
      },
      "outputs": [],
      "source": [
        "### Criação do sumário final, sumarizar com a transcrição do audio e descrição dos frames (deepseek)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UndAoGglDEld"
      },
      "source": [
        "Tentativa usando o ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jl4cYAjejFKf",
        "outputId": "c47db2e9-c33c-46bd-bece-9a623e7ae6ad"
      },
      "outputs": [],
      "source": [
        "### Tentativa de criação de contexto, criar uma descrição entre dois frames vizinhos (LLaMa)\n",
        "# Baseado em https://github.com/GoloMarcos/LLM4BrazilianFakeNews/blob/main/Evaluation.ipynb\n",
        "!pip install ollama\n",
        "import ollama\n",
        "\n",
        "# Fazer a analise entre a descrição de dois frames\n",
        "def analisaVizinhosLlama(model, desc1, desc2):\n",
        "  descricao1 = desc1['descricao']\n",
        "  frame1_time = desc1['frame'].split('_time_')[1].split('.')[0]\n",
        "  descricao2 = desc2['descricao']\n",
        "  frame2_time = desc2['frame'].split('_time_')[1].split('.')[0]\n",
        "\n",
        "  response = ollama.chat(model=model, messages=[{\n",
        "    'role': 'user',\n",
        "    'content': f'Considere as duas descrições em inglês a seguir e resuma-as em pt-br, contextualizando elas em conjunto. Explique como as duas se relacionam/diferenciam e o que elas podem significar em conjunto\\nDescrição 1: {descricao1} instante {frame1_time}\\nDescrição 2: {descricao2} instante {frame2_time}\\nSeu resumo deve integrar as duas descrições e proporcionar uma visão clara e coerente do contexto geral.'\n",
        "  },\n",
        "  ])\n",
        "  return response['message']['content']\n",
        "\n",
        "# Criar a lista com as novas descrições\n",
        "model = 'llama3:8b'\n",
        "def descricaoFramesVizinhosLlama(listDescricoesRecebida=None):\n",
        "  listDescricoesVizinhos = []\n",
        "  #listDescricoes = listDescricoesRecebida\n",
        "  for i in range(0, len(listDescricoesFrames) - 1, 2):\n",
        "    resposta = analisaVizinhosLlama(model, listDescricoesFrames[i], listDescricoesFrames[i+1])\n",
        "    if resposta:\n",
        "      # Armazenar as informações dos frames e a nova descrição\n",
        "      dictResposta = {\n",
        "          'frame1': listDescricoesFrames[i]['frame'],\n",
        "          'frame2': listDescricoesFrames[i+1]['frame'],\n",
        "          'descricao': resposta,\n",
        "      }\n",
        "      listDescricoesVizinhos.append(dictResposta)\n",
        "  # Com a lista criada:\n",
        "  if len(listDescricoesVizinhos) == len(listDescricoesFrames) - 1: # Todos os elementos tiveram a descrição criada\n",
        "    print('Descrição contextualizada criada!')\n",
        "    return listDescricoesVizinhos\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "\n",
        "if listDescricoesFrames:\n",
        "  ## receber\n",
        "  resultadoContexto = descricaoFramesVizinhosLlama(listDescricoesFrames)\n",
        "else:\n",
        "    print('Nenhuma descrição encontrada!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6IU534zsKMb"
      },
      "outputs": [],
      "source": [
        "### Sumarizar a parte visual do video (LLaMa)\n",
        "# Baseado em https://github.com/GoloMarcos/LLM4BrazilianFakeNews/blob/main/Evaluation.ipynb\n",
        "!pip install ollama\n",
        "import ollama\n",
        "\n",
        "# Fazer a analise entre a descrição de dois frames\n",
        "def analisaDescricoesFramesLlama(model, descricoes):\n",
        "  # Criação do prompt para todas as descrições\n",
        "  prompt = \"Considere as seguintes descrições e resuma-as, contextualizando todas em conjunto. Explique como elas se relacionam/diferenciam e o que elas podem significar em conjunto:\\n\"\n",
        "\n",
        "  # Adiconar cada descrição e sua respectiva timestamp\n",
        "  for i, desc in enumerate(descricoes):\n",
        "    #prompt += f\"\\nDescrição {i + 1}: {desc['descricao']} instante {desc['frame'].split('_time_')[1].split('.')[0]}\"\n",
        "    prompt += f\"\\nDescrição {i + 1}: {desc['descricao']} instante {desc['frame1'].split('_time_')[1].split('.')[0]} até instante {desc['frame2'].split('_time_')[1].split('.')[0]}\"\n",
        "\n",
        "  prompt += \"\\nSeu resumo deve integrar todas as descrições e fornecer uma visão clara e coerente do contexto geral.\"\n",
        "\n",
        "  response = ollama.chat(model=model, messages=[{\n",
        "    'role': 'user',\n",
        "    'content': prompt,\n",
        "  },\n",
        "  ])\n",
        "  return response['message']['content']\n",
        "\n",
        "# Criar a lista com as novas descrições\n",
        "model = 'llama3:8b'\n",
        "def analiseFramesLlama():\n",
        "  listDescricoesVizinhos = []\n",
        "  #listDescricoes = descricaoFrames() # Local já está com valor padrão correto\n",
        "  listDescricoes = descricaoFramesVizinhosLlama() # Tentar usar o texto contextualizado\n",
        "\n",
        "  if listDescricoes:\n",
        "    # Chama a função de análise para todas as descrições de uma vez\n",
        "    resposta = analisaDescricoesFramesLlama('llama3:8b', listDescricoes)\n",
        "    if resposta:\n",
        "      print('Descrição final dos frames criada!')\n",
        "    return resposta # Pode ser vazia\n",
        "  else:\n",
        "      return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2gDpgKfxcGw"
      },
      "outputs": [],
      "source": [
        "### Criação do sumário final, sumarizar com a transcrição do audio e descrição dos frames (LLaMa)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgXH2IfWxjS_"
      },
      "source": [
        "Possíveis APIs para a 4ª parte: uso de BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies), Hugging Face Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkyfa3V6zHEx"
      },
      "outputs": [],
      "source": [
        "# Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsghD7AOeB1p"
      },
      "source": [
        "5º Parte: Análise do sumário com dados do video"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
